{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.model_retrieval import ALBEF\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "\n",
    "import utils\n",
    "from dataset import create_dataset, create_sampler, create_loader\n",
    "from scheduler import create_scheduler\n",
    "from optim import create_optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    output_dir = './output/Retrieval_coco_small_0.05_vanila/'\n",
    "    checkpoint = './output/Retrieval_coco_small_0.05_vanila/checkpoint_4.pth'\n",
    "    text_encoder = 'bert-base-uncased'\n",
    "    device = 'cuda:1'\n",
    "    seed = 42 \n",
    "    world_size = 1 \n",
    "    \n",
    "@torch.no_grad()\n",
    "def itm_eval(scores_i2t, scores_t2i, txt2img, img2txt):\n",
    "    \n",
    "    #Images->Text \n",
    "    ranks = np.zeros(scores_i2t.shape[0])\n",
    "    for index,score in enumerate(scores_i2t):\n",
    "        inds = np.argsort(score)[::-1]\n",
    "        # Score\n",
    "        rank = 1e20\n",
    "        for i in img2txt[index]:\n",
    "            tmp = np.where(inds == i)[0][0]\n",
    "            if tmp < rank:\n",
    "                rank = tmp\n",
    "        ranks[index] = rank\n",
    "\n",
    "    # Compute metrics\n",
    "    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
    "  \n",
    "    #Text->Images \n",
    "    ranks = np.zeros(scores_t2i.shape[0])\n",
    "    \n",
    "    for index,score in enumerate(scores_t2i):\n",
    "        inds = np.argsort(score)[::-1]\n",
    "        ranks[index] = np.where(inds == txt2img[index])[0][0]\n",
    "\n",
    "    # Compute metrics\n",
    "    ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)        \n",
    "\n",
    "    tr_mean = (tr1 + tr5 + tr10) / 3\n",
    "    ir_mean = (ir1 + ir5 + ir10) / 3\n",
    "    r_mean = (tr_mean + ir_mean) / 2\n",
    "\n",
    "    eval_result =  {'txt_r1': tr1,\n",
    "                    'txt_r5': tr5,\n",
    "                    'txt_r10': tr10,\n",
    "                    'txt_r_mean': tr_mean,\n",
    "                    'img_r1': ir1,\n",
    "                    'img_r5': ir5,\n",
    "                    'img_r10': ir10,\n",
    "                    'img_r_mean': ir_mean,\n",
    "                    'r_mean': r_mean}\n",
    "    return eval_result\n",
    "    \n",
    "@torch.no_grad()\n",
    "def evaluation(model, data_loader, tokenizer, device, config):\n",
    "    # test\n",
    "    model.eval() \n",
    "    \n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Evaluation:'    \n",
    "    \n",
    "    print('Computing features for evaluation...')\n",
    "    start_time = time.time()  \n",
    "\n",
    "    texts = data_loader.dataset.text   \n",
    "    num_text = len(texts)\n",
    "    text_bs = 256\n",
    "    text_feats = []\n",
    "    text_embeds = []  \n",
    "    text_atts = []\n",
    "    for i in range(0, num_text, text_bs):\n",
    "        text = texts[i: min(num_text, i+text_bs)]\n",
    "        text_input = tokenizer(text, padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\").to(device) \n",
    "        with torch.no_grad():\n",
    "            text_output = model.text_encoder(text_input.input_ids, attention_mask = text_input.attention_mask, mode='text')  \n",
    "        text_feat = text_output.last_hidden_state\n",
    "        text_embed = F.normalize(model.text_proj(text_feat[:,0,:]))\n",
    "        \n",
    "        text_embeds.append(text_embed.detach().cpu())   \n",
    "        text_feats.append(text_feat.detach().cpu())\n",
    "        text_atts.append(text_input.attention_mask)\n",
    "        \n",
    "    text_embeds = torch.cat(text_embeds,dim=0)\n",
    "    text_feats = torch.cat(text_feats,dim=0)\n",
    "    text_atts = torch.cat(text_atts,dim=0)\n",
    "    \n",
    "    image_feats = []\n",
    "    image_embeds = []\n",
    "    for image, img_id in data_loader: \n",
    "        image = image.to(device) \n",
    "        with torch.no_grad():\n",
    "            image_feat = model.visual_encoder(image)        \n",
    "            image_embed = model.vision_proj(image_feat[:,0,:])            \n",
    "            image_embed = F.normalize(image_embed,dim=-1)      \n",
    "        \n",
    "        image_feats.append(image_feat.detach().cpu())\n",
    "        image_embeds.append(image_embed.detach().cpu())\n",
    "        \n",
    "        \n",
    "    image_feats = torch.cat(image_feats,dim=0)\n",
    "    image_embeds = torch.cat(image_embeds,dim=0)\n",
    "    \n",
    "    sims_matrix = image_embeds @ text_embeds.t()\n",
    "    score_matrix_i2t = torch.full((len(data_loader.dataset.image),len(texts)),-100.0)\n",
    "    \n",
    "    num_tasks = utils.get_world_size()\n",
    "    rank = utils.get_rank() \n",
    "    step = sims_matrix.size(0)//num_tasks + 1\n",
    "    start = rank*step\n",
    "    end = min(sims_matrix.size(0),start+step)\n",
    "\n",
    "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \n",
    "        \n",
    "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
    "        encoder_output = image_feats[start+i].repeat(config['k_test'],1,1).to(device)\n",
    "        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.text_encoder(encoder_embeds         = text_feats[topk_idx].to(device), \n",
    "                                        attention_mask         = text_atts[topk_idx].to(device),\n",
    "                                        encoder_hidden_states  = encoder_output.to(device),\n",
    "                                        encoder_attention_mask = encoder_att.to(device),                             \n",
    "                                        return_dict            = True,\n",
    "                                        mode                   = 'fusion'\n",
    "                                        )\n",
    "            \n",
    "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
    "        score_matrix_i2t[start+i,topk_idx] = score.detach().cpu()\n",
    "        \n",
    "    sims_matrix = sims_matrix.t()\n",
    "    score_matrix_t2i = torch.full((len(texts),len(data_loader.dataset.image)),-100.0)\n",
    "    \n",
    "    step = sims_matrix.size(0)//num_tasks + 1\n",
    "    start = rank*step\n",
    "    end = min(sims_matrix.size(0),start+step)    \n",
    "    \n",
    "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \n",
    "        \n",
    "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
    "        encoder_output     = image_feats[topk_idx].to(device)\n",
    "        encoder_att        = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.text_encoder(encoder_embeds         = text_feats[start+i].repeat(config['k_test'],1,1).to(device), \n",
    "                                        attention_mask         = text_atts[start+i].repeat(config['k_test'],1).to(device),\n",
    "                                        encoder_hidden_states  = encoder_output.to(device),\n",
    "                                        encoder_attention_mask = encoder_att.to(device),                             \n",
    "                                        return_dict            = True,\n",
    "                                        mode                   = 'fusion'\n",
    "                                        )\n",
    "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
    "        score_matrix_t2i[start+i,topk_idx] = score.detach().cpu()       \n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Evaluation time {}'.format(total_time_str)) \n",
    "\n",
    "    return score_matrix_i2t.cpu().numpy(), score_matrix_t2i.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### main ####\n",
    "config = yaml.load(open(os.path.join(args.output_dir,'config.yaml')),Loader=yaml.Loader)\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = args.device\n",
    "\n",
    "#dataset \n",
    "\n",
    "samplers = [None, None, None]\n",
    "train_dataset, val_dataset, test_dataset = create_dataset('re', config)  \n",
    "train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\n",
    "                                                          batch_size=[config['batch_size_train']]+[config['batch_size_test']]*2,\n",
    "                                                          num_workers=[0,0,0],\n",
    "                                                          is_trains=[True, False, False], \n",
    "                                                          collate_fns=[None,None,None])  \n",
    "\n",
    "# tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained(args.text_encoder)\n",
    "\n",
    "# Model \n",
    "model = ALBEF(config=config, text_encoder=args.text_encoder, tokenizer=tokenizer)\n",
    "\n",
    "# Model checkpoint \n",
    "checkpoint = torch.load(args.checkpoint, map_location='cpu') \n",
    "state_dict = checkpoint['model']\n",
    "pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)         \n",
    "state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)   \n",
    "state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped \n",
    "for key in list(state_dict.keys()):\n",
    "    if 'bert' in key:\n",
    "        encoder_key = key.replace('bert.','')         \n",
    "        state_dict[encoder_key] = state_dict[key] \n",
    "        del state_dict[key]                \n",
    "msg = model.load_state_dict(state_dict,strict=False)  \n",
    "\n",
    "print('load checkpoint from %s'%args.checkpoint)\n",
    "print(msg)  \n",
    "\n",
    "model = model.to(device)\n",
    "model_without_ddp = model\n",
    "score_val_i2t, score_val_t2i, = evaluation(model_without_ddp, val_loader, tokenizer, device, config)\n",
    "score_test_i2t, score_test_t2i = evaluation(model_without_ddp, test_loader, tokenizer, device, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'txt_r1': 90.633608815427, 'txt_r5': 99.72451790633609, 'txt_r10': 100.0, 'txt_r_mean': 96.78604224058769, 'img_r1': 82.43392070484582, 'img_r5': 98.29295154185021, 'img_r10': 99.39427312775331, 'img_r_mean': 93.37371512481644, 'r_mean': 95.07987868270206}\n",
      "{'txt_r1': 90.633608815427, 'txt_r5': 99.72451790633609, 'txt_r10': 100.0, 'txt_r_mean': 96.78604224058769, 'img_r1': 82.43392070484582, 'img_r5': 98.29295154185021, 'img_r10': 99.39427312775331, 'img_r_mean': 93.37371512481644, 'r_mean': 95.07987868270206}\n"
     ]
    }
   ],
   "source": [
    "val_result = itm_eval(score_val_i2t, score_val_t2i, val_loader.dataset.txt2img, val_loader.dataset.img2txt)  \n",
    "print(val_result)\n",
    "test_result = itm_eval(score_test_i2t, score_test_t2i, test_loader.dataset.txt2img, test_loader.dataset.img2txt)    \n",
    "print(test_result)\n",
    "\n",
    "epoch = 'eval'\n",
    "\n",
    "log_stats = {**{f'val_{k}': v for k, v in val_result.items()},\n",
    "                **{f'test_{k}': v for k, v in test_result.items()},                  \n",
    "            'epoch': epoch,\n",
    "            }\n",
    "with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
    "    f.write(json.dumps(log_stats) + \"\\n\")   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
