{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.model_retrieval import ALBEF\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "\n",
    "import utils\n",
    "from dataset import create_dataset, create_sampler, create_loader\n",
    "from scheduler import create_scheduler\n",
    "from optim import create_optimizer\n",
    "\n",
    "from augmentation.romixgen import BackTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    output_dir = './output/Retrieval_coco_small_0.05_vanila/'\n",
    "    checkpoint = './output/Retrieval_coco_small_0.05_vanila/checkpoint_4.pth'\n",
    "    text_encoder = 'bert-base-uncased'\n",
    "    device = 'cuda:1'\n",
    "    seed = 42 \n",
    "    world_size = 1 \n",
    "    \n",
    "@torch.no_grad()\n",
    "def itm_eval(scores_i2t, scores_t2i, txt2img, img2txt):\n",
    "    \n",
    "    #Images->Text \n",
    "    ranks = np.zeros(scores_i2t.shape[0])\n",
    "    for index,score in enumerate(scores_i2t):\n",
    "        inds = np.argsort(score)[::-1]\n",
    "        # Score\n",
    "        rank = 1e20\n",
    "        for i in img2txt[index]:\n",
    "            tmp = np.where(inds == i)[0][0]\n",
    "            if tmp < rank:\n",
    "                rank = tmp\n",
    "        ranks[index] = rank\n",
    "\n",
    "    # Compute metrics\n",
    "    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
    "  \n",
    "    #Text->Images \n",
    "    ranks = np.zeros(scores_t2i.shape[0])\n",
    "    \n",
    "    for index,score in enumerate(scores_t2i):\n",
    "        inds = np.argsort(score)[::-1]\n",
    "        ranks[index] = np.where(inds == txt2img[index])[0][0]\n",
    "\n",
    "    # Compute metrics\n",
    "    ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)        \n",
    "\n",
    "    tr_mean = (tr1 + tr5 + tr10) / 3\n",
    "    ir_mean = (ir1 + ir5 + ir10) / 3\n",
    "    r_mean = (tr_mean + ir_mean) / 2\n",
    "\n",
    "    eval_result =  {'txt_r1': tr1,\n",
    "                    'txt_r5': tr5,\n",
    "                    'txt_r10': tr10,\n",
    "                    'txt_r_mean': tr_mean,\n",
    "                    'img_r1': ir1,\n",
    "                    'img_r5': ir5,\n",
    "                    'img_r10': ir10,\n",
    "                    'img_r_mean': ir_mean,\n",
    "                    'r_mean': r_mean}\n",
    "    return eval_result\n",
    "    \n",
    "@torch.no_grad()\n",
    "def evaluation(model, data_loader, tokenizer, device, config):\n",
    "    # test\n",
    "    model.eval() \n",
    "    \n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Evaluation:'    \n",
    "    \n",
    "    print('Computing features for evaluation...')\n",
    "    start_time = time.time()  \n",
    "\n",
    "    texts = data_loader.dataset.text   \n",
    "    num_text = len(texts)\n",
    "    text_bs = 256\n",
    "    text_feats = []\n",
    "    text_embeds = []  \n",
    "    text_atts = []\n",
    "    for i in range(0, num_text, text_bs):\n",
    "        text = texts[i: min(num_text, i+text_bs)]\n",
    "        text_input = tokenizer(text, padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\").to(device) \n",
    "        with torch.no_grad():\n",
    "            text_output = model.text_encoder(text_input.input_ids, attention_mask = text_input.attention_mask, mode='text')  \n",
    "        text_feat = text_output.last_hidden_state\n",
    "        text_embed = F.normalize(model.text_proj(text_feat[:,0,:]))\n",
    "        \n",
    "        text_embeds.append(text_embed.detach().cpu())   \n",
    "        text_feats.append(text_feat.detach().cpu())\n",
    "        text_atts.append(text_input.attention_mask)\n",
    "        \n",
    "    text_embeds = torch.cat(text_embeds,dim=0)\n",
    "    text_feats = torch.cat(text_feats,dim=0)\n",
    "    text_atts = torch.cat(text_atts,dim=0)\n",
    "    \n",
    "    image_feats = []\n",
    "    image_embeds = []\n",
    "    for image, img_id in data_loader: \n",
    "        image = image.to(device) \n",
    "        with torch.no_grad():\n",
    "            image_feat = model.visual_encoder(image)        \n",
    "            image_embed = model.vision_proj(image_feat[:,0,:])            \n",
    "            image_embed = F.normalize(image_embed,dim=-1)      \n",
    "        \n",
    "        image_feats.append(image_feat.detach().cpu())\n",
    "        image_embeds.append(image_embed.detach().cpu())\n",
    "        \n",
    "        \n",
    "    image_feats = torch.cat(image_feats,dim=0)\n",
    "    image_embeds = torch.cat(image_embeds,dim=0)\n",
    "    \n",
    "    sims_matrix = image_embeds @ text_embeds.t()\n",
    "    score_matrix_i2t = torch.full((len(data_loader.dataset.image),len(texts)),-100.0)\n",
    "    \n",
    "    num_tasks = utils.get_world_size()\n",
    "    rank = utils.get_rank() \n",
    "    step = sims_matrix.size(0)//num_tasks + 1\n",
    "    start = rank*step\n",
    "    end = min(sims_matrix.size(0),start+step)\n",
    "\n",
    "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \n",
    "        \n",
    "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
    "        encoder_output = image_feats[start+i].repeat(config['k_test'],1,1).to(device)\n",
    "        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.text_encoder(encoder_embeds         = text_feats[topk_idx].to(device), \n",
    "                                        attention_mask         = text_atts[topk_idx].to(device),\n",
    "                                        encoder_hidden_states  = encoder_output.to(device),\n",
    "                                        encoder_attention_mask = encoder_att.to(device),                             \n",
    "                                        return_dict            = True,\n",
    "                                        mode                   = 'fusion'\n",
    "                                        )\n",
    "            \n",
    "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
    "        score_matrix_i2t[start+i,topk_idx] = score.detach().cpu()\n",
    "        \n",
    "    sims_matrix = sims_matrix.t()\n",
    "    score_matrix_t2i = torch.full((len(texts),len(data_loader.dataset.image)),-100.0)\n",
    "    \n",
    "    step = sims_matrix.size(0)//num_tasks + 1\n",
    "    start = rank*step\n",
    "    end = min(sims_matrix.size(0),start+step)    \n",
    "    \n",
    "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)): \n",
    "        \n",
    "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
    "        encoder_output     = image_feats[topk_idx].to(device)\n",
    "        encoder_att        = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.text_encoder(encoder_embeds         = text_feats[start+i].repeat(config['k_test'],1,1).to(device), \n",
    "                                        attention_mask         = text_atts[start+i].repeat(config['k_test'],1).to(device),\n",
    "                                        encoder_hidden_states  = encoder_output.to(device),\n",
    "                                        encoder_attention_mask = encoder_att.to(device),                             \n",
    "                                        return_dict            = True,\n",
    "                                        mode                   = 'fusion'\n",
    "                                        )\n",
    "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
    "        score_matrix_t2i[start+i,topk_idx] = score.detach().cpu()       \n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Evaluation time {}'.format(total_time_str)) \n",
    "\n",
    "    return score_matrix_i2t.cpu().numpy(), score_matrix_t2i.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from ./output/Retrieval_coco_small_0.05_vanila/checkpoint_4.pth\n",
      "<All keys matched successfully>\n",
      "Computing features for evaluation...\n",
      "Evaluation:  [  0/363]  eta: 0:02:59    time: 0.4938  data: 0.0004  max mem: 0\n",
      "Evaluation:  [ 50/363]  eta: 0:02:35    time: 0.4965  data: 0.0000  max mem: 0\n",
      "Evaluation:  [100/363]  eta: 0:02:10    time: 0.4997  data: 0.0000  max mem: 0\n",
      "Evaluation:  [150/363]  eta: 0:01:46    time: 0.4991  data: 0.0000  max mem: 0\n",
      "Evaluation:  [200/363]  eta: 0:01:21    time: 0.4993  data: 0.0000  max mem: 0\n",
      "Evaluation:  [250/363]  eta: 0:00:56    time: 0.4991  data: 0.0000  max mem: 0\n",
      "Evaluation:  [300/363]  eta: 0:00:31    time: 0.5003  data: 0.0000  max mem: 0\n",
      "Evaluation:  [350/363]  eta: 0:00:06    time: 0.4997  data: 0.0000  max mem: 0\n",
      "Evaluation:  [362/363]  eta: 0:00:00    time: 0.4995  data: 0.0000  max mem: 0\n",
      "Evaluation: Total time: 0:03:01 (0.4989 s / it)\n",
      "Evaluation:  [   0/1816]  eta: 0:15:36    time: 0.5159  data: 0.0021  max mem: 0\n",
      "Evaluation:  [  50/1816]  eta: 0:15:19    time: 0.5212  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 100/1816]  eta: 0:14:53    time: 0.5267  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 150/1816]  eta: 0:14:26    time: 0.5143  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 200/1816]  eta: 0:14:01    time: 0.5199  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 250/1816]  eta: 0:13:36    time: 0.5248  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 300/1816]  eta: 0:13:10    time: 0.5223  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 350/1816]  eta: 0:12:42    time: 0.5193  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 400/1816]  eta: 0:12:16    time: 0.5241  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 450/1816]  eta: 0:11:50    time: 0.5217  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 500/1816]  eta: 0:11:23    time: 0.5137  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 550/1816]  eta: 0:10:57    time: 0.5137  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 600/1816]  eta: 0:10:30    time: 0.5130  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 650/1816]  eta: 0:10:04    time: 0.5139  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 700/1816]  eta: 0:09:38    time: 0.5137  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 750/1816]  eta: 0:09:11    time: 0.5139  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 800/1816]  eta: 0:08:45    time: 0.5135  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 850/1816]  eta: 0:08:19    time: 0.5132  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 900/1816]  eta: 0:07:53    time: 0.5166  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 950/1816]  eta: 0:07:27    time: 0.5141  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1000/1816]  eta: 0:07:01    time: 0.5139  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1050/1816]  eta: 0:06:35    time: 0.5137  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1100/1816]  eta: 0:06:10    time: 0.5157  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1150/1816]  eta: 0:05:44    time: 0.5221  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1200/1816]  eta: 0:05:18    time: 0.5239  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1250/1816]  eta: 0:04:52    time: 0.5214  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1300/1816]  eta: 0:04:26    time: 0.5178  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1350/1816]  eta: 0:04:01    time: 0.5139  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1400/1816]  eta: 0:03:35    time: 0.5170  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1450/1816]  eta: 0:03:09    time: 0.5226  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1500/1816]  eta: 0:02:43    time: 0.5183  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1550/1816]  eta: 0:02:17    time: 0.5136  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1600/1816]  eta: 0:01:51    time: 0.5133  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1650/1816]  eta: 0:01:25    time: 0.5142  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1700/1816]  eta: 0:00:59    time: 0.5133  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1750/1816]  eta: 0:00:34    time: 0.5139  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1800/1816]  eta: 0:00:08    time: 0.5144  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1815/1816]  eta: 0:00:00    time: 0.5136  data: 0.0000  max mem: 0\n",
      "Evaluation: Total time: 0:15:38 (0.5169 s / it)\n",
      "Evaluation time 0:18:48\n",
      "Computing features for evaluation...\n",
      "Evaluation:  [  0/363]  eta: 0:03:00    time: 0.4982  data: 0.0004  max mem: 0\n",
      "Evaluation:  [ 50/363]  eta: 0:02:36    time: 0.4992  data: 0.0000  max mem: 0\n",
      "Evaluation:  [100/363]  eta: 0:02:12    time: 0.5162  data: 0.0000  max mem: 0\n",
      "Evaluation:  [150/363]  eta: 0:01:47    time: 0.4999  data: 0.0000  max mem: 0\n",
      "Evaluation:  [200/363]  eta: 0:01:22    time: 0.5068  data: 0.0000  max mem: 0\n",
      "Evaluation:  [250/363]  eta: 0:00:57    time: 0.4994  data: 0.0000  max mem: 0\n",
      "Evaluation:  [300/363]  eta: 0:00:31    time: 0.5062  data: 0.0000  max mem: 0\n",
      "Evaluation:  [350/363]  eta: 0:00:06    time: 0.5088  data: 0.0000  max mem: 0\n",
      "Evaluation:  [362/363]  eta: 0:00:00    time: 0.5061  data: 0.0000  max mem: 0\n",
      "Evaluation: Total time: 0:03:03 (0.5068 s / it)\n",
      "Evaluation:  [   0/1816]  eta: 0:16:36    time: 0.5488  data: 0.0013  max mem: 0\n",
      "Evaluation:  [  50/1816]  eta: 0:15:09    time: 0.5144  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 100/1816]  eta: 0:14:50    time: 0.5245  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 150/1816]  eta: 0:14:25    time: 0.5209  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 200/1816]  eta: 0:14:00    time: 0.5196  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 250/1816]  eta: 0:13:34    time: 0.5259  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 300/1816]  eta: 0:13:07    time: 0.5129  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 350/1816]  eta: 0:12:42    time: 0.5193  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 400/1816]  eta: 0:12:15    time: 0.5131  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 450/1816]  eta: 0:11:48    time: 0.5133  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 500/1816]  eta: 0:11:21    time: 0.5137  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 550/1816]  eta: 0:10:55    time: 0.5135  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 600/1816]  eta: 0:10:28    time: 0.5133  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 650/1816]  eta: 0:10:02    time: 0.5136  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 700/1816]  eta: 0:09:36    time: 0.5130  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 750/1816]  eta: 0:09:10    time: 0.5134  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 800/1816]  eta: 0:08:44    time: 0.5134  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 850/1816]  eta: 0:08:18    time: 0.5133  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 900/1816]  eta: 0:07:52    time: 0.5139  data: 0.0000  max mem: 0\n",
      "Evaluation:  [ 950/1816]  eta: 0:07:27    time: 0.5308  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1000/1816]  eta: 0:07:01    time: 0.5233  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1050/1816]  eta: 0:06:36    time: 0.5266  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1100/1816]  eta: 0:06:10    time: 0.5191  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1150/1816]  eta: 0:05:44    time: 0.5174  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1200/1816]  eta: 0:05:18    time: 0.5196  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1250/1816]  eta: 0:04:52    time: 0.5141  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1300/1816]  eta: 0:04:26    time: 0.5155  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1350/1816]  eta: 0:04:00    time: 0.5138  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1400/1816]  eta: 0:03:35    time: 0.5132  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1450/1816]  eta: 0:03:09    time: 0.5138  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1500/1816]  eta: 0:02:43    time: 0.5132  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1550/1816]  eta: 0:02:17    time: 0.5138  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1600/1816]  eta: 0:01:51    time: 0.5137  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1650/1816]  eta: 0:01:25    time: 0.5139  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1700/1816]  eta: 0:00:59    time: 0.5137  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1750/1816]  eta: 0:00:34    time: 0.5145  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1800/1816]  eta: 0:00:08    time: 0.5137  data: 0.0000  max mem: 0\n",
      "Evaluation:  [1815/1816]  eta: 0:00:00    time: 0.5133  data: 0.0000  max mem: 0\n",
      "Evaluation: Total time: 0:15:37 (0.5162 s / it)\n",
      "Evaluation time 0:18:50\n"
     ]
    }
   ],
   "source": [
    "#### main ####\n",
    "config = yaml.load(open(os.path.join(args.output_dir,'config.yaml')),Loader=yaml.Loader)\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = args.device\n",
    "\n",
    "#dataset \n",
    "\n",
    "samplers = [None, None, None]\n",
    "train_dataset, val_dataset, test_dataset = create_dataset('re', config)  \n",
    "train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\n",
    "                                                          batch_size=[config['batch_size_train']]+[config['batch_size_test']]*2,\n",
    "                                                          num_workers=[0,0,0],\n",
    "                                                          is_trains=[True, False, False], \n",
    "                                                          collate_fns=[None,None,None])  \n",
    "\n",
    "# tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained(args.text_encoder)\n",
    "\n",
    "# Model \n",
    "model = ALBEF(config=config, text_encoder=args.text_encoder, tokenizer=tokenizer)\n",
    "\n",
    "# Model checkpoint \n",
    "checkpoint = torch.load(args.checkpoint, map_location='cpu') \n",
    "state_dict = checkpoint['model']\n",
    "pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)         \n",
    "state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)   \n",
    "state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped \n",
    "for key in list(state_dict.keys()):\n",
    "    if 'bert' in key:\n",
    "        encoder_key = key.replace('bert.','')         \n",
    "        state_dict[encoder_key] = state_dict[key] \n",
    "        del state_dict[key]                \n",
    "msg = model.load_state_dict(state_dict,strict=False)  \n",
    "\n",
    "print('load checkpoint from %s'%args.checkpoint)\n",
    "print(msg)  \n",
    "\n",
    "model = model.to(device)\n",
    "model_without_ddp = model\n",
    "score_val_i2t, score_val_t2i, = evaluation(model_without_ddp, val_loader, tokenizer, device, config)\n",
    "score_test_i2t, score_test_t2i = evaluation(model_without_ddp, test_loader, tokenizer, device, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'txt_r1': 90.633608815427, 'txt_r5': 99.72451790633609, 'txt_r10': 100.0, 'txt_r_mean': 96.78604224058769, 'img_r1': 82.43392070484582, 'img_r5': 98.29295154185021, 'img_r10': 99.39427312775331, 'img_r_mean': 93.37371512481644, 'r_mean': 95.07987868270206}\n",
      "{'txt_r1': 90.633608815427, 'txt_r5': 99.72451790633609, 'txt_r10': 100.0, 'txt_r_mean': 96.78604224058769, 'img_r1': 82.43392070484582, 'img_r5': 98.29295154185021, 'img_r10': 99.39427312775331, 'img_r_mean': 93.37371512481644, 'r_mean': 95.07987868270206}\n"
     ]
    }
   ],
   "source": [
    "val_result = itm_eval(score_val_i2t, score_val_t2i, val_loader.dataset.txt2img, val_loader.dataset.img2txt)  \n",
    "print(val_result)\n",
    "test_result = itm_eval(score_test_i2t, score_test_t2i, test_loader.dataset.txt2img, test_loader.dataset.img2txt)    \n",
    "print(test_result)\n",
    "\n",
    "epoch = 4 \n",
    "\n",
    "log_stats = {**{f'val_{k}': v for k, v in val_result.items()},\n",
    "                **{f'test_{k}': v for k, v in test_result.items()},                  \n",
    "            'epoch': epoch,\n",
    "            }\n",
    "with open(os.path.join(args.output_dir, \"log.txt\"),\"w\") as f:\n",
    "    f.write(json.dumps(log_stats) + \"\\n\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "result = [] \n",
    "ratio = [0.01,0.05]\n",
    "method = ['romixgen','vanila']\n",
    "for r in ratio:\n",
    "    for m in method:\n",
    "        line = open(f'./output/Retrieval_coco_small_{r}_{m}/log.txt').readline()\n",
    "        line = ast.literal_eval(line)\n",
    "        result.append({f'{r}_{m}' : line})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,res in enumerate(result):\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame.from_dict(res,orient='index')\n",
    "    else:\n",
    "        df = pd.concat([df,pd.DataFrame.from_dict(res,orient='index')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = json.load(open(config['train_file'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28337"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
