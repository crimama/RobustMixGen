{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/workspace'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import RoMixGen_Img, RoMixGen_Txt\n",
    "import random\n",
    "from PIL import Image \n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform_after_mix = transforms.Compose([\n",
    "                                            transforms.RandomResizedCrop(256,scale=(0.5, 1.0), interpolation=Image.BICUBIC),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RoMixGen_Img = RoMixGen_Img(image_dict = json.load(open('data/COCO/Annotations/coco_img_info_0.01_bbox_center_0.7.json')),\n",
    "                            image_root = 'data/COCO/Images_0.01_bbox_center_0.7',\n",
    "                            transform_after_mix = transform_after_mix,\n",
    "                            resize_ratio = 2)\n",
    "RoMixGen_Txt = RoMixGen_Txt(image_caption = json.load(open('data/COCO/Annotations/coco_img_info_0.01_bbox_center_0.7.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('data/COCO/Annotations/coco_objbg_info_0.01_bbox_center_0.7.json') as f:\n",
    "    objbg_dict = json.load(f)\n",
    "with open ('data/COCO/Annotations/coco_img_info_0.01_bbox_center_0.7.json') as f:\n",
    "    image_dict = json.load(f)\n",
    "with open ('data/COCO/Annotations/coco.json') as f:\n",
    "    original_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 123,287 romixed images and captions\n",
    "# new image id start from 000001 ~ 123287\n",
    "# save image to data/COCO/pre_romix_coco_0.01_bbox_center_0.7\n",
    "# save caption to data/COCO/Anotations/pre_romix_caption_0.01_bbox_center_0.7.json\n",
    "total_imagenum = 123287\n",
    "obj_ids = objbg_dict['obj']\n",
    "bg_ids = objbg_dict['bg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/123287 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  0%|          | 288/123287 [04:40<33:14:03,  1.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m txt \u001b[39m=\u001b[39m RoMixGen_Txt(obj_id, bg_id)\n\u001b[1;32m     18\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpermute(img, (\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     19\u001b[0m img \u001b[39m=\u001b[39m (img \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(img)) \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mmax(img) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(img))\n",
      "File \u001b[0;32m/workspace/augmentation/romixgen.py:210\u001b[0m, in \u001b[0;36mRoMixGen_Txt.__call__\u001b[0;34m(self, obj_id, bg_id)\u001b[0m\n\u001b[1;32m    207\u001b[0m bg_caption \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_caption[bg_id][\u001b[39m\"\u001b[39m\u001b[39mcaptions\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    209\u001b[0m new_caption \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplace_word(bg_caption_item, bg_cat, obj_cat) \u001b[39mfor\u001b[39;00m bg_caption_item \u001b[39min\u001b[39;00m bg_caption]\n\u001b[0;32m--> 210\u001b[0m backtranslated_text \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mback_translate(new_caption_item) \u001b[39mfor\u001b[39;00m new_caption_item \u001b[39min\u001b[39;00m new_caption]\n\u001b[1;32m    211\u001b[0m \u001b[39m#backtranslated_text = self.back_translate(new_caption) \u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m backtranslated_text\n",
      "File \u001b[0;32m/workspace/augmentation/romixgen.py:210\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m bg_caption \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_caption[bg_id][\u001b[39m\"\u001b[39m\u001b[39mcaptions\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    209\u001b[0m new_caption \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplace_word(bg_caption_item, bg_cat, obj_cat) \u001b[39mfor\u001b[39;00m bg_caption_item \u001b[39min\u001b[39;00m bg_caption]\n\u001b[0;32m--> 210\u001b[0m backtranslated_text \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mback_translate(new_caption_item) \u001b[39mfor\u001b[39;00m new_caption_item \u001b[39min\u001b[39;00m new_caption]\n\u001b[1;32m    211\u001b[0m \u001b[39m#backtranslated_text = self.back_translate(new_caption) \u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m backtranslated_text\n",
      "File \u001b[0;32m/workspace/augmentation/romixgen.py:181\u001b[0m, in \u001b[0;36mRoMixGen_Txt.back_translate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    179\u001b[0m first_tokenized          \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_model_tokenizer(first_formed_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    180\u001b[0m first_tokenized          \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m first_tokenized\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 181\u001b[0m first_translated         \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfirst_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfirst_tokenized)\n\u001b[1;32m    182\u001b[0m first_translated_text    \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_model_tokenizer\u001b[39m.\u001b[39mdecode(first_translated[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    183\u001b[0m second_formed_text       \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>>en<< \u001b[39m\u001b[39m{\u001b[39;00mfirst_translated_text\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1474\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1468\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1469\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1470\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1471\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1475\u001b[0m         input_ids,\n\u001b[1;32m   1476\u001b[0m         beam_scorer,\n\u001b[1;32m   1477\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1478\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1479\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1480\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1481\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1482\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1483\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1484\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1485\u001b[0m     )\n\u001b[1;32m   1487\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1488\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:2789\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m beam_next_tokens \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_tokens\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2787\u001b[0m beam_idx \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_indices\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m-> 2789\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([input_ids[beam_idx, :], beam_next_tokens\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   2791\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2792\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   2793\u001b[0m )\n\u001b[1;32m   2794\u001b[0m \u001b[39mif\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classic_dataset = []\n",
    "new_dataset = {}\n",
    "\n",
    "if not os.path.exists('data/COCO/pre_romix_coco_0.01_bbox_center_0.7'):\n",
    "    os.makedirs('data/COCO/pre_romix_coco_0.01_bbox_center_0.7')\n",
    "\n",
    "for img_id in tqdm(range(total_imagenum)):\n",
    "    # Keep retrying until the image generation succeeds\n",
    "    while True:\n",
    "        obj_id, bg_id = str(random.choice(obj_ids)), str(random.choice(bg_ids))\n",
    "        try:\n",
    "            img = RoMixGen_Img(obj_id, bg_id)\n",
    "            break # exit the loop if image generation succeeds\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    txt = RoMixGen_Txt(obj_id, bg_id)\n",
    "    img = torch.permute(img, (1,2,0)).detach().numpy()\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "    img = img*255\n",
    "\n",
    "    # Write image to disk\n",
    "    img_path = os.path.join('data/COCO/pre_romix_coco_0.01_bbox_center_0.7', f'{img_id:06d}.jpg')\n",
    "    if not os.path.exists(img_path):\n",
    "        cv2.imwrite(img_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Add captions to datasets\n",
    "    classic_dataset.extend([{'caption': _, 'image': img_path, 'image_id': f\"coco_{img_id:06d}\"} for _ in txt])\n",
    "    new_dataset.setdefault(str(img_id), []).extend(txt)\n",
    "\n",
    "    # dump datasets\n",
    "    if img_id % 100 == 0:\n",
    "        with open('data/COCO/Annotations/pre_romix_caption_0.01_bbox_center_0.7.json', 'w') as f:\n",
    "            json.dump(classic_dataset, f)\n",
    "        with open('data/COCO/Annotations/pre_romix_caption_new_0.01_bbox_center_0.7.json', 'w') as f:\n",
    "            json.dump(new_dataset, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
